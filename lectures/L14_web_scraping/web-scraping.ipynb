{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b5d3db",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "-   The internet is full of data\n",
    "-   Much of our lives and livelihoods lives on the internet\n",
    "-   The programming and theoretical skills we've developed can allow us\n",
    "    to do meaningful analysis of internet data\n",
    "\n",
    "… If we can get it into Python!\n",
    "\n",
    "## Accessing Website Data\n",
    "\n",
    "-   Some data we see on websites data is easier to access:\n",
    "    -   Provided as a downloadable file we can `pd.read_csv`\n",
    "    -   Accessible via an API\n",
    "    -   Contained in a clean html table for `pd.read_html`\n",
    "-   … but some of it isn't\n",
    "-   Today we'll learn how to access some of the data that is publicly\n",
    "    visible, but not easy to download\n",
    "\n",
    "## HTML: Language of the web\n",
    "\n",
    "-   Web pages are written in a markup language called HTML\n",
    "\n",
    "-   HTML stands for \"hyper text markup language\"\n",
    "\n",
    "-   All HTML documents are composed of a tree of nested elements\n",
    "\n",
    "-   For example, this bullet point list would be written like this in\n",
    "    HTML:\n",
    "\n",
    "    ``` html\n",
    "    <ul>\n",
    "      <li>Web pages are..</li>\n",
    "      <li>HTML stands for...</li>\n",
    "      <li>All HTML documents...</li>\n",
    "      <li>For example, this...</li>\n",
    "    </ul>\n",
    "    ```\n",
    "\n",
    "### Components of HTML\n",
    "\n",
    "-   Below is an image that annotates the core parts of an HTML document\n",
    "\n",
    "![](attachment:./html_parts.png)\n",
    "\n",
    "-   Tag: name or type of an element\n",
    "-   CSS classes: used to style and change appearance (we'll use it to\n",
    "    identify specific elements!)\n",
    "-   id: Unique identifier for element **on whole webpage**\n",
    "-   value: `class` is one property, syntax is `property\\=value`\n",
    "-   text: the actual text contained in the element\n",
    "\n",
    "### Structure of Webpage\n",
    "\n",
    "-   Most webpages follow a very common structure:\n",
    "\n",
    "    ``` html\n",
    "    <!DOCTYPE HTML>\n",
    "    <html>\n",
    "    <head>\n",
    "      <meta>...</meta>\n",
    "      <title>...</title>\n",
    "      <link>...</link>\n",
    "    </head>\n",
    "    <body>\n",
    "      <h1>Title</h1>\n",
    "      .... MANY MORE ELEMENTS HERE ...\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "\n",
    "-   Almost all the data we will want to scrape is contained inside the\n",
    "    `<body>` element\n",
    "\n",
    "### See it in Action!\n",
    "\n",
    "-   Let's see this in action\n",
    "-   We'll navigate to\n",
    "    [<http://quotes.toscrape.com/random>](http://quotes.toscrape.com/random)\n",
    "    and use our web browser to look at the HTML\n",
    "-   Things to look for:\n",
    "    -   The outline from previous slide\n",
    "    -   The use of `class`\n",
    "    -   The hierarchy of the page\n",
    "\n",
    "### Multiple Quotes?\n",
    "\n",
    "-   Now that we are warmed up, let's look at a page with multiple quotes\n",
    "-   Navigate to\n",
    "    [<http://quotes.toscrape.com/>](http://quotes.toscrape.com/) and\n",
    "    look at the source\n",
    "-   We'll watch for the same main concepts/components\n",
    "\n",
    "### How to \"Scrape\"?\n",
    "\n",
    "-   Now the main question: \"How could we scrape this data?\"\n",
    "-   The key to web scraping is to be able to **identify** patterns\n",
    "-   My main strategy for doing this is to follow these steps:\n",
    "    1.  View the webpage and identify visually the data I'd like to\n",
    "        scrape\n",
    "    2.  Open browser tools and \"inspect\" the element containing my data\n",
    "    3.  Look at that element's tag, classes, id to see how I could tell\n",
    "        a computer to identify it\n",
    "    4.  Look outwards to other elements to scrape\n",
    "        -   Same type of data, e.g. a price (find pattern in structure\n",
    "            that matches original element)\n",
    "        -   Different type of data, e.g. an average review: start\n",
    "            process again\n",
    "\n",
    "# Scrapy\n",
    "\n",
    "-   There are many Python libraries for scraping websites\n",
    "-   Perhaps the most widely used of these is called `scrapy`\n",
    "-   We will use `scrapy` to extract the quote information we saw on the\n",
    "    example websites\n",
    "-   First step would be to install scrapy if you haven't yet:\n",
    "    `pip install scrapy`\n",
    "\n",
    "## Running Scrapy\n",
    "\n",
    "-   Scrapy can be run in the `scrapy shell` as we just saw\n",
    "-   However, one benefit from learning how to scrape websites is to be\n",
    "    able to have scrapers run as programs that don't need manual\n",
    "    interactions\n",
    "-   Scrapy was built for this use case\n",
    "\n",
    "## Scrapy Project\n",
    "\n",
    "-   Scrapy provides a scaffold we can use to organize our web scrapers\n",
    "-   This is called a scrapy project\n",
    "-   We can create one by running `scrapy startproject NAME` where `NAME`\n",
    "    is the name of our project\n",
    "-   Let's try it!\n",
    "\n",
    "## Spiders\n",
    "\n",
    "-   We need to teach scrapy how to extract data from the web pages we\n",
    "    have it visit\n",
    "-   To do this we create a Spider\n",
    "-   A spider is a class we define that has at least the following\n",
    "    features:\n",
    "    -   A list of websites to scrape\n",
    "    -   A Python function for how to exract data from a single webpage\n",
    "-   We'll create our first spider now\n",
    "\n",
    "## Working with Spiders\n",
    "\n",
    "-   We can run a spider using `scrapy crawl NAME -o OUTFILE.EXT`, where\n",
    "    -   `NAME` is the name of the spider\n",
    "    -   `OUTFILE.EXT` are the name and extension for storing the data\n",
    "-   We can also have the spider continue on to another page\n",
    "    -   To do this we need to find the next url to visit and tell scrapy\n",
    "        to scrape it\n",
    "    -   To scrape the next url we use the `response.follow(URL)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
