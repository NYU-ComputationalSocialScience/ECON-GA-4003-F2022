{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Lake Model meets US data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the setting and some standard language that helps us keep sight of how various\n",
    "pieces of our analysis fit together.\n",
    "\n",
    "* A **statistical model** is a joint probability density $f(Y; \\theta)$ for a sequence $Y$ of random variables indexed by a list $\\theta$ of parameters.\n",
    "\n",
    "* The **direct problem** is to draw a random sample $y$ from the statistical model\n",
    "$f(Y; \\theta)$ for an assumed value of the parameter vector $\\theta$.\n",
    "\n",
    "* The **inverse problem** is to take a set of data $\\tilde y$ assumed to be drawn from\n",
    "$f(Y;\\theta)$ and to use them to make inferences about the unknown parameter vector $\\theta$.\n",
    "\n",
    "Thus, the **direct problem** is to construct a **simulation** of model $f(Y;\\theta)$\n",
    "for a given $\\theta$.  \n",
    "\n",
    "* Other names for  the simulated data $ y$ are *artificial data* or *fake data*\n",
    "\n",
    "The **inverse problem** is also called the **parameter estimation problem**.\n",
    "\n",
    "The **direct problem** takes parameters as inputs and artificial or fake data as outputs\n",
    "\n",
    "The **inverse problem** takes real world data (i.e., \"the observations\") as inputs and statements about parameters as outputs.\n",
    "\n",
    "Both problems assume the same statistical model $f(Y; \\theta)$.\n",
    "\n",
    "\n",
    "Tools that allow us to solve the **direct problem** often turn out to be helpful in \n",
    "solving the **inverse problem**.\n",
    "\n",
    "\n",
    "**Application to lake model**\n",
    "\n",
    "Please keep this framework in mind as we use various versions of the  \"lake model\"\n",
    "to try to understand employment and unemployment rates over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recent unemployment rates in the US\n",
    "\n",
    "The BLS data show a large shift in the employment/unemployment numbers in the US in early 2020\n",
    "\n",
    "Let's remind ourselves what these data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bls = pd.read_csv(\"bls_recessions_data.csv\")\n",
    "df_bls[\"labor_supply\"] = df_bls.eval(\"employed + unemployed\")\n",
    "df_bls[\"employed_pct\"] = df_bls.eval(\"employed / labor_supply\")\n",
    "df_bls[\"unemployed_pct\"] = df_bls.eval(\"unemployed / labor_supply\")\n",
    "df_bls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_recession_data(df):\n",
    "    \"\"\"\n",
    "    Plot the employment and unemployment percents contained in df\n",
    "    \n",
    "    The x value is `months_from` with interpretation of the months\n",
    "    from the trough of a recession. A negative number indicates a \n",
    "    date before the trough\n",
    "    \n",
    "    There will be subplots for employment and unemployment percentages\n",
    "    \n",
    "    There will be one line per subplot for each unique value of the \n",
    "    `recession` column\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    df: pd.DataFrame\n",
    "        A pandas DataFrame containing the labor market data to be plotted\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fig: matplotlib.pyplot.Figure\n",
    "        A matplotlib Figure with the chart\n",
    "    \"\"\"\n",
    "    # create figure with two subplots, one for emp, one for unemp\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # list of columns to plot\n",
    "    columns = [\"employed_pct\", \"unemployed_pct\"]\n",
    "    for ax, col in zip(axs, columns):        \n",
    "        # loop over data and plot column on axes for each recession\n",
    "        for recession, data in df.groupby(\"recession\"):\n",
    "            data.plot(x=\"months_from\", y=col, ax=ax, label=recession)\n",
    "        # beautify axes\n",
    "        ax.set_title(col)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.set_xlabel(\"Months from trough\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_recession_data(df_bls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've been using a lake model with two parameters:\n",
    "\n",
    "- $\\alpha$: probability of unemployed worker finding a job. Called the *job finding rate*\n",
    "- $\\beta$: probability of employed worker losing a job. Called the *job separation rate*\n",
    "\n",
    "And one state variable:\n",
    "\n",
    "- $s$: a  $2 \\times 1$ vector of  percentages of unemployed and employed workers, respectively.\n",
    "\n",
    "With constant values  of $\\alpha$ and $\\beta$, our model cannot generate sudden large changes in the unemployment rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at an example using code from `v1_employment_model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v1_employment_model as emp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will first create a new Python class that we'll use throughout this lesson\n",
    "\n",
    "The class will help us:\n",
    "\n",
    "- Keep track of model parameters\n",
    "- Simulate a panel of workers\n",
    "- Plot simulated results alongside data from BLS\n",
    "\n",
    "The class looks long, but most of the lines are docstrings and comments\n",
    "\n",
    "We'll review it one step at a time and it will be clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LakeModelSimulator:\n",
    "    def __init__(self, N:int = 5000, s_0: np.ndarray=np.array([0.0237, 0.9763])):\n",
    "        \"\"\"\n",
    "        Helper class that can simulate individuals working in an economy\n",
    "        where job loss and finding is governed by the Lake model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        N: int\n",
    "            The number of individuals in the simulated panels\n",
    "        s_0: np.ndarray(float, ndim=1, size=2)\n",
    "            The initial distribution of [unemployed, employed] workers\n",
    "            in the economy. These should be two numbers between [0, 1]\n",
    "            that sum to one. The default value is the long run value of the\n",
    "            employment to unemployment ratio computed using data from the\n",
    "            CPS\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self._s_0 = s_0\n",
    "    \n",
    "    # define s_0 as Python property so we can ensure it cannot be \n",
    "    # set to an inappropriate value\n",
    "    \n",
    "    # define a `get_s_0` method. lm.s_0\n",
    "    def get_s_0(self) -> np.ndarray:\n",
    "        return self._s_0\n",
    "        \n",
    "    # and a set_s_0 method\n",
    "    def set_s_0(self, new_s_0: np.ndarray):\n",
    "        \"\"\"\n",
    "        Change the value of s_0\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        new_s_0: np.ndarray\n",
    "            The new value of s_0 that should be used in future simulations\n",
    "        \"\"\"\n",
    "        assert len(new_s_0) == 2\n",
    "        assert abs(sum(new_s_0) - 1.0) < 1e-10\n",
    "        self._s_0 = np.asarray(new_s_0)\n",
    "    \n",
    "    # combine the two to create the official s_0\n",
    "    s_0 = property(get_s_0, set_s_0)\n",
    "    \n",
    "    def combine_with_bls_data(self, df_sim: pd.DataFrame, df_bls: pd.DataFrame=df_bls) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Combine the simulated data from `df_sim` with the BLS data in\n",
    "        `df_bls` and plot the percent of workers employed and unemployed\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df_sim: pd.DataFrame\n",
    "            The simulated DataFrame constructed by this class\n",
    "        \n",
    "        df_bls: pd.DataFrame\n",
    "            The DataFrame containing official data from the BLS\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        df: pandas.DataFrame\n",
    "            A DataFrame containing the combined data\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Both DataFrames should have at least the following columns:\n",
    "        [\"months_from\", \"employed_pct\", \"unemployed_pct\", \"recession\"]\n",
    "        \n",
    "        Also, the `months_from` column in `df_sim` will be modified as\n",
    "        `df_sim[\"months_from\"] -= df_bls[\"months_from\"].min()` to align\n",
    "        dates\n",
    "        \"\"\"\n",
    "        # adjust the months_from on the simulated data by -35 to match\n",
    "        # the bls data\n",
    "        df_sim_plot = df_sim.copy()\n",
    "        df_sim_plot[\"months_from\"] -= 35\n",
    "        df = pd.concat([df_sim_plot, df_bls], ignore_index=True)\n",
    "        return df\n",
    "    \n",
    "    def simulate_panel(\n",
    "            self,\n",
    "            alpha: np.ndarray, \n",
    "            beta: np.ndarray,\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Simulate a panel of employees whose transitions in and out\n",
    "        of employment are goverened by the lake model with parameters\n",
    "        alpha and beta\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: np.array(float, ndim=1)\n",
    "            The probability that an individual goes from\n",
    "            unemployed to employed\n",
    "        beta: np.array(float, ndim=1)\n",
    "            The probability that an individual goes from\n",
    "            employed to unemployed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.DataFrame\n",
    "            A pandas DataFrame in the same form as the `df_bls` DataFrame\n",
    "            from above. In particular, it has columns \n",
    "            [\"months_from\", \"employed_pct\", \"unemployed_pct\", \"recession\"]\n",
    "        \"\"\"\n",
    "        # workers is N x T array of (0, 1)\n",
    "        workers = emp.simulate_employment_cross_section(\n",
    "            alpha, beta, self.s_0, N=self.N\n",
    "        )\n",
    "        T = workers.shape[1]\n",
    "\n",
    "        # sum over all workers to get count of \n",
    "        # how many of the 500 workers were employed in each period\n",
    "        employed = workers.sum(axis=0)\n",
    "\n",
    "        # put into dataframe with columns expected by the plot function\n",
    "        return (\n",
    "            pd.DataFrame(dict(\n",
    "                months_from=np.arange(T),\n",
    "                employed_pct=employed/self.N,\n",
    "                unemployed_pct=1 - employed/self.N,\n",
    "            )).assign(recession=\"model\")\n",
    "        )\n",
    "    \n",
    "    def simulate_panel_fixed_alpha_beta(\n",
    "            self,\n",
    "            alpha_0: float, \n",
    "            beta_0: float, \n",
    "            T:int = 72,\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Helper function to simulate Lake model when alpha and beta\n",
    "        are constant\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float\n",
    "            The probability that an individual goes from\n",
    "            unemployed to employed\n",
    "        beta: float\n",
    "            The probability that an individual goes from\n",
    "            employed to unemployed\n",
    "        T: int\n",
    "            The number of periods for which to simulate. by default this \n",
    "            is 60 (to match length of 'gr' subset of `df_bls`)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.DataFrame\n",
    "            A pandas DataFrame in the same form as the `df_bls` DataFrame\n",
    "            from above. In particular, it has columns \n",
    "            [\"months_from\", \"employed_pct\", \"unemployed_pct\", \"recession\"]\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "\n",
    "        See also the method `simulate_employment_panel_df`\n",
    "        \"\"\"\n",
    "        alpha_vec = np.ones(T) * alpha_0\n",
    "        beta_vec = np.ones(T) * beta_0\n",
    "        return self.simulate_panel(alpha_vec, beta_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's try out this class with reasonable \"normal time\" parameters for $\\alpha$ and $\\beta$\n",
    "\n",
    "We'll call these normal-time parameters $\\bar{\\alpha} = 0.37$ and $\\bar{\\beta} = 0.01$ and refer to them as the steady state parameters\n",
    "\n",
    "These values were estimated using CPS data in an earlier lecture\n",
    "\n",
    "Also note that we'll start with $s_0 = \\bar{s} = [0.0237, 0.9763]$, which are implied steady state percentages of unemployed and employed workers (also estimated previously using CPS data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_bar = 0.37\n",
    "beta_bar = 0.01\n",
    "s_bar = np.array([0.0237, 0.9763])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LakeModelSimulator(s_0=np.array([0.2, 0.8]), N=150000)\n",
    "df_sim = lm.combine_with_bls_data(\n",
    "    lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar)\n",
    ") \n",
    "plot_recession_data(df_sim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Visually, we can see that the steady state version of this model does not match the features of either recession, which are:\n",
    "\n",
    "- The COVID recession has an extremely sharp movement in the employment distribution\n",
    "- The great recession featured a prolonged departure from pre-crisis levels\n",
    "\n",
    "Our goal in this lecture is to discover deviations from $\\alpha_t = \\bar{\\alpha}, \\beta_t = \\bar{\\beta}\\; \\forall t$ that will allow the Lake model to do a better job of describing  times of stress in the US labor market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss functions\n",
    "\n",
    "Before looking at different assumptions about $\\{\\alpha_t, \\beta_t \\}$, we will first posit a **loss function**\n",
    "\n",
    "A loss function is a deterministic function that measures the difference between a model's output and its target\n",
    "\n",
    "The loss function is a key component to any statistical optimization routine and is heavily used  machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use a very common loss function called the mean-squared-error loss function, or MSE\n",
    "\n",
    "Given a sequence of targets and model outputs $\\{y_i, \\hat{y}_i\\}_{i=1}^N$ the MSE is defined as\n",
    "\n",
    "$$MSE(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our current exercise, the model output will be the time series of percentage of workers that are employed\n",
    "\n",
    "The target will be the correspoinding time-series from the BLS\n",
    "\n",
    "We will compute the loss function indpendently for the great recession and COVID recession times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mse(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with columns [\"months_from\", \"recession\", and \"employed_pct\"],\n",
    "    compute the MSE between model output and both great recession and COVID era\n",
    "    unemployment data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        A DataFrame containing the unemployment BLS data and model output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mse: pandas.Series\n",
    "        A pandas Series with `mse_gr` and `mse_covid` representing the MSE between\n",
    "        model output and great recession data and COVID era data, repsectively\n",
    "    \"\"\"\n",
    "    # set index so pandas will align data for us. Also multiply by 100 to move into\n",
    "    # percentage units. This rescaling will make the MSE larger and easier to compare\n",
    "    # across models\n",
    "    _df = df.pivot(index=\"months_from\", columns=\"recession\", values=\"employed_pct\") * 100\n",
    "    return pd.Series(dict(\n",
    "        mse_gr=_df.eval(\"(model - gr)**2\").mean(),\n",
    "        mse_covid=_df.eval(\"(model - covid)**2\").mean()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mse(df_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the loss function will allow us to move from qualitative to quantitative statements about the goodness of fit\n",
    "\n",
    "For example, instead of saying \n",
    "\n",
    "> Model 2 appears to do better than model 1 at matching the spike in COVID era unemployment\n",
    "\n",
    "we could say \n",
    "\n",
    "> The `mse_covid` for model 2 is 5.1 relative to 11.5 for model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model 1: one time shock to $s_t$\n",
    "\n",
    "Consider the hypothesis that the COVID-19 spike in unemployment was only a shift of the employment/unemployment distribution ($s_0$), but didn't actually cause changes to the job creation ($\\alpha$) or separation ($\\beta$) rates\n",
    "\n",
    "In term of our model parameters, this means\n",
    "\n",
    "- $\\alpha_t = \\bar{\\alpha}, \\beta_t = \\bar{\\beta}\\; \\forall t$\n",
    "- $s_0 = \\bar{s}$, $s_t = \\text{\"shocked\" } s$, where $t$ is date of shock\n",
    "- $s_{\\tau}$ with $\\tau > t$ is dictated by $\\bar{\\alpha}$ and $\\bar{\\beta}$\n",
    "\n",
    "> Note: for $\\tau >> t$ we will have $s_{\\tau} = \\bar{s}$ again. This is what is meant by **steady state**, i.e.,  a place of rest according to the internal dynamics of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have all the tools we need  to  inspect this hypothesis visually!\n",
    "\n",
    "We'll follow these steps:\n",
    "\n",
    "\n",
    "1. Set $s_0 = \\bar{s}$\n",
    "1. Set $\\alpha_t = \\bar{\\alpha}, \\beta_t = \\bar{\\beta}\\; \\forall t$\n",
    "1. Simulate for first 35 periods (for $t = -35, \\dots -1$)\n",
    "1. At $t = 0$ set $s_t = [0.14, 0.86]$ which is approximately the peak level of unemployment in spring 2020 due to COVID-19\n",
    "1. Simulate for $t = 0, \\dots, 36$ starting from that point\n",
    "1. Combine the 2 halfs of the simulation and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_single_shock(\n",
    "        lm: LakeModelSimulator, \n",
    "        s_t: np.ndarray=np.array([0.14, 0.86])\n",
    "    ):\n",
    "    # Step 1 -- make sure s_0 is s_bar\n",
    "    lm.s_0 = s_bar\n",
    "    \n",
    "    # Step 2 and 3 -- simulate for first 35 periods\n",
    "    df_before = lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar, T=34)\n",
    "    \n",
    "    # Step 4 -- \"shock\" s_t. Note we do this by changing s_0\n",
    "    # we'll adjust for the difference between timing `0` and `t` after\n",
    "    # simulation\n",
    "    lm.s_0 = s_t\n",
    "    \n",
    "    # Step 5 -- simulate for remainder of 35 periods\n",
    "    df_after = lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar, T=35)\n",
    "    df_after[\"months_from\"] += 35\n",
    "    \n",
    "    # Step 6 -- combine simulation 1/2s\n",
    "    out = pd.concat([df_before, df_after], ignore_index=True)  \n",
    "    return lm.combine_with_bls_data(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_shock = simulate_single_shock(lm)\n",
    "fig = plot_recession_data(df_shock);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some things to notice:\n",
    "\n",
    "- Up until the period after the shock, the model and COVID lines line up very closely\n",
    "- The model seems to recover even faster than the data suggests the COVID recovery is happening\n",
    "- The model recovers *much* faster than the great recession recovery, which says more about the difference between teh two recessions than anything else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The loss function for this model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mse(df_shock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might have expected, `mse_covid < mse_gr` because we forced the model to match the most extreme value of the COVID era recession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model 2: Single shock to $s_t$ and temporary shifts in $\\alpha$, $\\beta$\n",
    "\n",
    "We just learned that our model is recovering even faster than actual COVID-era data\n",
    "\n",
    "Perhaps the assumption that $\\alpha$ and $\\beta$ reamined constant is too strong..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now relax that assumption, but in a disciplined way\n",
    "\n",
    "We will assume that until the $t=0$ shock, $\\alpha_t$ and $\\beta_t$ are at their steady-state levels\n",
    "\n",
    "Then, we assume that $s_t$ immediately jumps to the spiked COVID levels\n",
    "\n",
    "Finally, we assume that  starting at zero and continuing temporarily for $N$ (a finite number of) periods, $\\alpha$ and $\\beta$ move to new values $\\hat{\\alpha}$ and $\\hat{\\beta}$\n",
    "\n",
    "After  $N$ periods, $\\alpha$ and $\\beta$ return to $\\bar{\\alpha}$ and $\\bar{\\beta}$\n",
    "\n",
    "Mathematically, these assumptions can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha_t &= \\begin{cases}  \\hat{\\alpha} & 0 \\le t \\le N \\\\ \\bar{\\alpha} & \\; \\text{ else } \\end{cases} \\\\\n",
    "\\beta_t &= \\begin{cases} \\hat{\\beta} & 0 \\le t \\le N \\\\ \\bar{\\beta} & \\; \\text{ else } \\end{cases} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_alpha_beta_shock(\n",
    "        lm: LakeModelSimulator,\n",
    "        alpha_hat: float, \n",
    "        beta_hat: float, \n",
    "        N: int,\n",
    "        s_t: np.ndarray=np.array([0.14, 0.86])\n",
    "    ):\n",
    "    # pre t=0 remains the same...\n",
    "    lm.s_0 = s_bar\n",
    "    df_before = lm.simulate_panel_fixed_alpha_beta(alpha_bar, beta_bar, T=34)\n",
    "    \n",
    "    # still shock at t=0\n",
    "    lm.s_0 = s_t\n",
    "    \n",
    "    # prepare alpha_vec and beta_vec for t >=0 simulation\n",
    "    alpha_vec = np.ones(35) * alpha_bar\n",
    "    alpha_vec[:N] = alpha_hat\n",
    "    \n",
    "    beta_vec = np.ones(35) * beta_bar\n",
    "    beta_vec[:N] = beta_hat\n",
    "    \n",
    "    # simulate using vector of alpha and beta\n",
    "    df_after = lm.simulate_panel(alpha_vec, beta_vec)\n",
    "    df_after[\"months_from\"] += 35\n",
    "    \n",
    "    # combine simulation 1/2 and return\n",
    "    out = pd.concat([df_before, df_after], ignore_index=True)\n",
    "    return lm.combine_with_bls_data(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have this routine, let's test a version of the model where the job finding rate ($\\alpha$) falls from 0.37 to 0.28  and the job separation rate ($\\beta$) rises from 0.01 to 0.05\n",
    "\n",
    "We'll suppose that this lasts for 3 periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_beta = simulate_alpha_beta_shock(lm, 0.28, 0.05, 3)\n",
    "plot_recession_data(df_alpha_beta);\n",
    "compute_mse(df_alpha_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our initial test values of $\\hat{\\alpha}$, $\\hat{\\beta}$, and $N$ did improve the `mse_covid` from about 4.0 to about 2.6\n",
    "\n",
    "We chose these values informally (only using our intuition about what might have happened at the peak of the COVID era spike)\n",
    "\n",
    "There are other values of these parameters that could have been better (lower `mse_covid`)\n",
    "\n",
    "Let's assume the role of the \"visual econometrician\" and approximate superior values visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def explore_alpha_beta_shock(alpha_hat: float, beta_hat:float , N: int):\n",
    "    df_alpha_beta = simulate_alpha_beta_shock(lm, alpha_hat, beta_hat, N)\n",
    "    plot_recession_data(df_alpha_beta);\n",
    "    return compute_mse(df_alpha_beta)\n",
    "\n",
    "interact(\n",
    "    explore_alpha_beta_shock, \n",
    "    alpha_hat=FloatSlider(value=0.28, min=0.2, max=0.5, step=0.025),\n",
    "    beta_hat=FloatSlider(value=0.05, min=0.01, max=0.2, step=0.01),\n",
    "    N=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our experimenting, we found the following values to trace the \"COVID recovery\" quite well\n",
    "\n",
    "- $\\hat{\\alpha} = 0.25$\n",
    "- $\\hat{\\beta} = 0.02$\n",
    "- $N = 4$\n",
    "\n",
    "The `mse_covid` fell to about 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model 3: shock to $\\alpha_{t-1}$ and/or $\\beta_{t-1}$\n",
    "\n",
    "In our experiments thus far, we have moved $s_0$ by hand\n",
    "\n",
    "We might want to answer the question, \"What change in $\\alpha$ and $\\beta$ at period t-1 could have generated the drop in employment seen at time 0?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $\\alpha_{-1}$ or $\\beta_{-1}$?\n",
    "\n",
    "In principle, either a decrease in the job finding rate or an increase in the job separation rate could lead to a decrease in the percentage of employed workers\n",
    "\n",
    "Let's look specifically at the BLS data to for COVID era `months_from` -1 and 0 to identify the magnitude of the change in employment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bls[[\"months_from\", \"recession\", \"employed_pct\"]].query(\"months_from in [-1, 0] and recession=='covid'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Attempt 1: change $\\alpha_{-1}$\n",
    "\n",
    "Recall the steady-state job finding rate $\\bar{\\alpha}=0.37$ and job separation rate $\\bar{\\beta} = 0.01$\n",
    "\n",
    "Suppose we believe that the change the employment data was driven entirely by a drop in the finding rate\n",
    "\n",
    "Let's set $\\alpha_{-1}$ to 0 and see what impact that has on $s_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of employed workers at \"m\"inus 1\n",
    "emp_m = 0.955\n",
    "\n",
    "# move forward this percentage one period -- setting alpha to 0\n",
    "emp_0_alpha = emp_m * (1-beta_bar) + 0 * (1-emp_m)\n",
    "\n",
    "print(\"emp_0 when beta = beta_bar and alpha = 0:\", emp_0_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is far greater than the 0.855 from the BLS data\n",
    "\n",
    "This shows that within the context of our model, the hypothesis that the drop in employment could not have come entirely from the job finding rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Attempt 2: Change $\\beta_{-1}$\n",
    "\n",
    "Let's now consider a change in the job separation rate $\\beta_{-1}$\n",
    "\n",
    "Suppose we set this to 1 and keep $\\alpha_{-1} = \\bar{\\alpha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# move forward this percentage one period -- setting alpha to 0\n",
    "emp_0_beta = emp_m * (1-1) + alpha_bar * (1-emp_m)\n",
    "\n",
    "print(\"emp_0 when beta_{-1} = 0 and alpha_{-1} = alpha_bar:\", emp_0_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This extreme value in the job separation rate puts time 0 employment down at 1.7%\n",
    "\n",
    "This validates that it is *possible* (again within the context of our model) for a change in the job separation rate to be consistent with the BLS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now answer the question: \"Keeping $\\alpha_{-1} = \\bar{\\alpha}$, what value of $\\beta_{-1}$ could generate a shift from \n",
    "0.954 of workers employed to 0.8555 workers employed?\"\n",
    "\n",
    "We'll call this value $\\tilde{\\beta}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can re-arrange the law of motion for the fraction of employed workers to solve for $\\tilde{\\beta}$:\n",
    "\n",
    "$$\\tilde{\\beta} = 1 - \\frac{e_0 - \\bar{\\alpha}(1-e_{-1})}{e_{-1}},$$\n",
    "\n",
    "Where $e_t$ represents fraction of workers employed at time $t$\n",
    "\n",
    "Let's compute this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_0 = 0.8556\n",
    "beta_tilde = 1 - (emp_0 - alpha_bar*(1 - emp_m)) / emp_m\n",
    "print(\"beta_tilde is: \", beta_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This suggests that in order for our model to generate the drop in employment observed in the COVID era, the job separation rate would need to move from the steady state value of 0.01 to 0.12\n",
    "\n",
    "Thus, 12% of workers lost their jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's now simulate our model where we assume a one time shock to $\\beta_{-1}$ and then an immediate return to $\\bar{\\beta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_time_beta_shock(\n",
    "        lm: LakeModelSimulator, \n",
    "        beta_tilde: float\n",
    "    ):\n",
    "    # create vectors of alpha and beta\n",
    "    lm.s_0 = s_bar\n",
    "    alpha_vec = np.ones(71) * alpha_bar\n",
    "    beta_vec = np.ones(71) * beta_bar\n",
    "    \n",
    "    # move only beta_{t-1} to beta_tilde\n",
    "    beta_vec[34] = beta_tilde\n",
    "    \n",
    "    # simulate\n",
    "    out = lm.simulate_panel(alpha_vec, beta_vec)\n",
    "    return lm.combine_with_bls_data(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_beta_shock = simulate_one_time_beta_shock(lm, beta_tilde)\n",
    "plot_recession_data(df_beta_shock)\n",
    "compute_mse(df_beta_shock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This looks very similar to the original model in which supposed a one time shock to $s_0$...why?\n",
    "\n",
    "We found the value of $\\beta_{-1}$ that caused $s_0$ to equal the shocked value \n",
    "\n",
    "Then, we kept $\\beta_t$ ($t \\not= -1$) and $\\alpha_t$ equal to their steady-state values\n",
    "\n",
    "We allowed the model to *generate* the change in $s_0$ instead of imposing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the model outputs are the same, why could this be a desirable outcome?\n",
    "\n",
    "Becuase the change in $s_0$ is coming from within the model, we can now do experiments like:\n",
    "\n",
    "- Suppose the government could have saved 1/3 of the lost jobs in time 0. What impact would that have had on $s_t$, $t >=0$\n",
    "\n",
    "In this sense we are able to use the model as a vehicle for doing counterfactual exercises\n",
    "\n",
    "This idea captures the general spirit of computational social science: we build models that can capture a feature(s) of actual data we'd like to better understand so that we can analyze the impact of decisions on these variables of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model 4: Shock $\\beta_{-1}$ and $\\alpha_{t}$, $\\beta_{t}$\n",
    "\n",
    "In model 3 we were able to replicate the spike in COVID era unemployment\n",
    "\n",
    "In model 2 (where we moved $\\alpha_t$ and $\\beta_{t}\\; t = 0, \\dots N$) we were able to replicate the first few periods of the recovery\n",
    "\n",
    "Let's combine these two to have a single model that can match the COVID era labor market dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def shock_beta_m_and_alpha_beta(\n",
    "        lm: LakeModelSimulator,\n",
    "        beta_tilde: float,\n",
    "        alpha_hat: float, \n",
    "        beta_hat: float, \n",
    "        N: int,\n",
    "    ):\n",
    "    # create vectors of alpha and beta\n",
    "    lm.s_0 = s_bar\n",
    "    alpha_vec = np.ones(71) * alpha_bar\n",
    "    beta_vec = np.ones(71) * beta_bar\n",
    "    \n",
    "    # move beta_{t-1} to beta_tilde\n",
    "    beta_vec[34] = beta_tilde\n",
    "    \n",
    "    # move alpha_0 to alpha_N to alpha_hat\n",
    "    alpha_vec[35:(35+N)] = alpha_hat\n",
    "    \n",
    "    # same for beta\n",
    "    beta_vec[35:(35+N)] = beta_hat\n",
    "    \n",
    "    out = lm.simulate_panel(alpha_vec, beta_vec)\n",
    "    return lm.combine_with_bls_data(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll use the values found above: \n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{\\beta} &= 0.12 \\\\\n",
    "\\hat{\\beta} &= 0.02 \\\\ \n",
    "\\hat{\\alpha} &= 0.25 \\\\\n",
    "N &= 4\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_betam_and_alpha_beta = shock_beta_m_and_alpha_beta(lm, beta_tilde, 0.25, 0.02, 4)\n",
    "plot_recession_data(df_betam_and_alpha_beta)\n",
    "compute_mse(df_betam_and_alpha_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model 5: Time varying $\\alpha_t$ and $\\beta_t$\n",
    "\n",
    "Our analysis has focused on models where $\\alpha$ and $\\beta$ are at their steady state values, except for a few periods\n",
    "\n",
    "This may be reasonable for the very sharp COVID era activity, but less reasonable for the great recession era\n",
    "\n",
    "The great recession featured a prolonged, gradual increase in unemployment and then a slow decline\n",
    "\n",
    "In order for our model to achieve something like this, we would need to allow $\\alpha_t$ and $\\beta_t$ to be different from their steady state values for multiple periods\n",
    "\n",
    "Let's now turn to the question of \"What values of $\\alpha_t$ and $\\beta_t$, for $t \\in [-35, 35]$ could generate great recession era dynamics?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `df_bls` DataFrame has columns `ee`, `eu`, `ue`, and `uu`\n",
    "\n",
    "These represent rates of individuals moving between employment and unemployment status (based on first letter of each word)\n",
    "\n",
    "The `ue` and `eu` map very closely into our $\\alpha$ and $\\beta$\n",
    "\n",
    "We'll treat them as a time series of $\\alpha_t$ and $\\beta_t$, respectively as we simulate our Lake model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_great_recession(\n",
    "        lm: LakeModelSimulator,\n",
    "        df_bls: pd.DataFrame,\n",
    "        s_init: np.ndarray,\n",
    "    ):\n",
    "    df_gr = df_bls.query(\"recession == 'gr'\").sort_values(\"months_from\")\n",
    "    alpha_vec = df_gr[\"ue\"].to_numpy()\n",
    "    beta_vec = df_gr[\"eu\"].to_numpy()\n",
    "    lm.s_0 = s_init\n",
    "    out = lm.simulate_panel(alpha_vec, beta_vec)\n",
    "    return lm.combine_with_bls_data(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial state for great recession times\n",
    "s_init_gr = [0.05, 0.95]\n",
    "df_model_gr = simulate_great_recession(lm, df_bls, s_init_gr)\n",
    "plot_recession_data(df_model_gr)\n",
    "compute_mse(df_model_gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice now that our model can track the labor market dynamics in teh great recession era quite well; `mse_gr` is down to 0.51\n",
    "\n",
    "However the `mse_covid` has risen to 9.05\n",
    "\n",
    "This illustrates a common curse (and blessing!) to all modeling exercsies: modeling decisions comes with tradeoffs\n",
    "\n",
    "Our two state markov chain view of labor market fluctuations cannot match both the great recession era and the COVID era\n",
    "\n",
    "The blessing in disguise is\n",
    "\n",
    "- We must be delibrate in specifying our modeling goals: do we want to match great recession era dynamics or COVID era dynamics?\n",
    "- We can afford to use simpler models:  we want the simplest model that allows us to achieve the modeling goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking ahead\n",
    "\n",
    "Why go through the trouble of building models?\n",
    "\n",
    "As social scientists, we rarely have the luxury of setting up controlled experiments\n",
    "\n",
    "The nature of our subject of study -- society and people -- does not allow for easy, controlled testing of hypothesis\n",
    "\n",
    "So, in order to employ the scientific method, we must create for ourselves a laboratory\n",
    "\n",
    "We use laboratories constructed of mathematical equation and statistical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A model allows us to consider \"what if\" scenarios, often called counterfactuals\n",
    "\n",
    "In these scenarios we can evaluate the tradeoffs of potential policy decisions\n",
    "\n",
    "For example, our Lake model could be the labor market piece of a larger economic framework\n",
    "\n",
    "This larger framework could include \n",
    "\n",
    "- Household decisions like saving vs. spending\n",
    "- A government that must raise taxes and enact policies for labor market relief\n",
    "- International trading partners that can increase diversity of goods and smooth out labor shocks\n",
    "\n",
    "Building up a rich framework by connecting well-understood pieces allows us to make a change in one component (e.g. government spending to subsidise labor market) and see the corresponding impact on other parts of the economy (e.g. a lower separation rate and increased household consumption/welfare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Throughout this course we will put our data, programming, math, modeling tools to work to build models and perform experiments like this"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "06e05088bf2d2704501f452c5673235c92421ea24b381cad1d147a1ece3057ad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
